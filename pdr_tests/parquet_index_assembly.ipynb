{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820aa942",
   "metadata": {},
   "source": [
    "This notebook contains a basic workflow for generating manifest parquet files from csv files generated by our scrapers. Depending on how much RAM you have available, it may not be appropriate for really painfully huge csv files; they may require special handling to split them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa \n",
    "from pyarrow import parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcabbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path you're storing csv files and manifests in\n",
    "manifest_folder = Path(os.getcwd(), \"node_manifests\")\n",
    "# size-corrected scraper output csv, relative to the manifest folder\n",
    "scraper_file = 'geolunar_size_corrected.csv'\n",
    "# 100000 is generally a pretty good row group size for these files as \n",
    "# a balance between loading speed and memory efficiency. If you run \n",
    "# into a situation where you really need to optimize this, you're probably\n",
    "# better-off just splitting up the manifest files.\n",
    "row_group_size = 100000\n",
    "# name of the output manifest file you'll be creating\n",
    "manifest_filename = f'{Path(scraper_file).stem.replace(\"_size_corrected\", \"\")}.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scraper file into memory\n",
    "df = pd.read_csv(\n",
    "    Path(manifest_folder, scraper_file), \n",
    "    header = 0, \n",
    "    dtype = {'url': str, 'size': str, 'units': str},\n",
    "    names = ('url', 'size', 'units')\n",
    ")\n",
    "# find and print missing-size entries -- these may represent\n",
    "# files that weren't hit correctly by the spider/scraper, or files\n",
    "# that didn't provide size info in their headers. this is usually\n",
    "# harmless, but in some cases might not be.\n",
    "missing = df.loc[\n",
    "    np.logical_or(\n",
    "        df['size'] == 'ErrorLogged',\n",
    "        df['size'].isna()\n",
    "    )\n",
    "]\n",
    "print(missing['url'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim missing-size entries and units column\n",
    "df = df.drop(missing.index)\n",
    "df = df.drop(columns='units')\n",
    "df = df.loc[df['size'] != 'ErrorLogged'].reset_index(drop=True)\n",
    "df['size'] = df['size'].astype(int)\n",
    "# chop off protocol string \n",
    "df['url'] = df['url'].str.replace(\"http://\", \"\")\n",
    "df['url'] = df['url'].str.replace(\"https://\", \"\")\n",
    "# split url to parts by '/'; keep the first part (domain) \n",
    "# and the last (filename) separate from the rest (url)\n",
    "fn_url = df['url'].str.rsplit('/', n=1, expand=True)\n",
    "domain_url = fn_url[0].str.split('/', n=1, expand=True)\n",
    "# reassemble all this into the search-efficient field structure \n",
    "# used in the manifest files\n",
    "df[['domain', 'url']] = domain_url\n",
    "del domain_url\n",
    "df['filename'] = fn_url[1]\n",
    "del fn_url\n",
    "df = df.reindex(columns=['domain', 'url', 'filename', 'size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write that table to a parquet file\n",
    "parquet.write_table(\n",
    "    pa.Table.from_pandas(df, preserve_index=False),\n",
    "    Path(\"node_manifests\", manifest_filename),\n",
    "    version='2.6',\n",
    "    row_group_size=row_group_size,\n",
    "    use_dictionary=['domain', 'url', 'size']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc72267",
   "metadata": {},
   "source": [
    "### STOP HERE (unless you want to combine parquet files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b6bf8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# should you wish to concatenate multiple parquet files\n",
    "tabs = [\n",
    "    parquet.read_table(f'node_manifests/img_jpl_msl_{ds}.parquet')\n",
    "    for ds in [\n",
    "        'navcam', \n",
    "        'hazcam', \n",
    "        'mahli', \n",
    "        \"mastcam\", \n",
    "        \"mrd\",\n",
    "        \"etc\"\n",
    "    ]\n",
    "]\n",
    "bigtab = pa.concat_tables(tabs)\n",
    "bigname = 'img_jpl_msl.parquet'\n",
    "parquet.write_table(\n",
    "    bigtab, \n",
    "    Path(\"node_manifests\", bigname),\n",
    "    row_group_size=row_group_size,\n",
    "    version = \"2.6\",\n",
    "    use_dictionary = ['domain', 'url', 'size']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
